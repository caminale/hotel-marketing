#!/usr/bin/env python3
"""
=============================================================================
SCRAPER RETROCALAGE.COM - Annuaire des Clubs
=============================================================================

Strat√©gie :
1. Selenium : ouvre le site, clique sur "Afficher plus" jusqu'√† √©puisement
2. R√©cup√®re le HTML complet
3. Selenium se ferme
4. BeautifulSoup : extrait les donn√©es de chaque club

Usage:
    python3 scrape_retrocalage.py

=============================================================================
"""

import csv
import re
import time
import sys
from datetime import datetime

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"‚ùå Module manquant: {e}")
    print("   pip3 install selenium beautifulsoup4")
    sys.exit(1)


# =============================================================================
# CONFIGURATION
# =============================================================================

URL = "https://retrocalage.com/clubs?mode=list"
OUTPUT_FILE = "bdd_club/auto/retrocalage.csv"


# =============================================================================
# SELENIUM - Charger toutes les donn√©es
# =============================================================================

def load_all_clubs():
    """
    Ouvre le site, clique sur 'Afficher plus' jusqu'√† ce qu'il n'y en ait plus,
    puis retourne le HTML complet.
    """
    print("=" * 60)
    print("üöó SCRAPER RETROCALAGE.COM")
    print("=" * 60)
    print()
    
    print("üåê Lancement de Selenium...")
    
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--window-size=1920,1080')
    
    driver = webdriver.Chrome(options=options)
    
    try:
        print(f"üìÑ Chargement de {URL}")
        driver.get(URL)
        
        # Attendre que la page charge
        time.sleep(3)
        
        click_count = 0
        
        print("üîÑ Clic sur 'Afficher plus' jusqu'√† √©puisement...")
        
        while True:
            try:
                # Chercher le bouton "Afficher plus"
                # On essaie plusieurs s√©lecteurs possibles
                button = None
                
                # Essayer diff√©rents s√©lecteurs
                selectors = [
                    "//button[contains(text(), 'Afficher plus')]",
                    "//a[contains(text(), 'Afficher plus')]",
                    "//button[contains(text(), 'afficher plus')]",
                    "//a[contains(text(), 'afficher plus')]",
                    "//button[contains(@class, 'load-more')]",
                    "//a[contains(@class, 'load-more')]",
                    "//button[contains(text(), 'Voir plus')]",
                    "//a[contains(text(), 'Voir plus')]",
                ]
                
                for selector in selectors:
                    try:
                        button = driver.find_element(By.XPATH, selector)
                        if button.is_displayed():
                            break
                        button = None
                    except NoSuchElementException:
                        continue
                
                if button is None:
                    print(f"\n‚úÖ Plus de bouton 'Afficher plus' trouv√© apr√®s {click_count} clics")
                    break
                
                # Scroller jusqu'au bouton
                driver.execute_script("arguments[0].scrollIntoView(true);", button)
                time.sleep(0.5)
                
                # Cliquer
                button.click()
                click_count += 1
                
                print(f"   Clic #{click_count}...", end="\r")
                
                # Attendre le chargement
                time.sleep(1.5)
                
            except Exception as e:
                print(f"\n‚úÖ Fin du chargement apr√®s {click_count} clics ({type(e).__name__})")
                break
        
        print()
        print("üì• R√©cup√©ration du HTML...")
        html = driver.page_source
        
        return html
        
    finally:
        print("üîí Fermeture de Selenium")
        driver.quit()


# =============================================================================
# BEAUTIFULSOUP - Extraire les donn√©es
# =============================================================================

def extract_clubs(html):
    """
    Parse le HTML et extrait les informations de chaque club.
    """
    print()
    print("üîç Analyse du HTML avec BeautifulSoup...")
    
    soup = BeautifulSoup(html, 'html.parser')
    clubs = []
    
    # Sauvegarder le HTML pour debug si besoin
    with open('retrocalage_debug.html', 'w', encoding='utf-8') as f:
        f.write(html)
    print("   üíæ HTML sauvegard√© dans retrocalage_debug.html pour debug")
    
    # Chercher les cartes de clubs
    # On va d'abord identifier la structure
    
    # Essayer diff√©rents s√©lecteurs pour trouver les clubs
    club_cards = soup.find_all('div', class_=re.compile(r'club|card', re.I))
    
    if not club_cards:
        # Essayer de trouver par la structure
        club_cards = soup.find_all('article')
    
    if not club_cards:
        # Chercher des liens ou divs contenant les infos
        club_cards = soup.find_all('div', class_=re.compile(r'list|item', re.I))
    
    print(f"   Trouv√© {len(club_cards)} √©l√©ments potentiels")
    
    # Analyser la structure pour trouver le bon pattern
    # On va chercher des patterns communs: nom, adresse, t√©l√©phone, email
    
    # M√©thode alternative: chercher tous les √©l√©ments avec des patterns reconnaissables
    # Emails
    email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}')
    # T√©l√©phones fran√ßais
    phone_pattern = re.compile(r'(?:0|\+33)[1-9](?:[\s.-]?\d{2}){4}')
    
    # Chercher la structure r√©elle
    # D'abord, trouvons le premier club mentionn√©: TEAM MOBYLETTE SALUC√âENS
    first_club = soup.find(string=re.compile(r'TEAM MOBYLETTE', re.I))
    
    if first_club:
        print(f"   ‚úÖ Premier club trouv√©: {first_club.strip()[:50]}...")
        # Remonter pour trouver le conteneur parent
        parent = first_club.parent
        for _ in range(5):
            if parent and parent.parent:
                parent = parent.parent
        if parent:
            print(f"   Structure parent: {parent.name}, classes: {parent.get('class', [])}")
    
    # Chercher tous les clubs par leur structure
    # On va chercher des √©l√©ments qui contiennent les infos de contact
    
    all_text = soup.get_text()
    
    # Compter les emails trouv√©s pour avoir une id√©e du nombre de clubs
    emails_found = email_pattern.findall(all_text)
    print(f"   üìß {len(emails_found)} emails trouv√©s dans la page")
    
    # Strat√©gie: trouver tous les conteneurs qui ont un titre (h2, h3, h4) 
    # suivi d'infos de contact
    
    # Chercher les sections de clubs
    sections = []
    
    # Pattern 1: Chercher par titres
    for heading in soup.find_all(['h2', 'h3', 'h4', 'h5']):
        section = {
            'nom': heading.get_text(strip=True),
            'adresse': '',
            'representant': '',
            'telephone': '',
            'email': '',
            'site': ''
        }
        
        # Chercher les infos dans les √©l√©ments suivants
        container = heading.find_parent(['div', 'article', 'section', 'li'])
        if container:
            text = container.get_text()
            
            # Email
            email_match = email_pattern.search(text)
            if email_match:
                section['email'] = email_match.group()
            
            # T√©l√©phone
            phone_match = phone_pattern.search(text)
            if phone_match:
                section['telephone'] = phone_match.group()
            
            # Site web
            for link in container.find_all('a', href=True):
                href = link['href']
                if href.startswith('http') and 'retrocalage' not in href and 'mailto:' not in href:
                    section['site'] = href
                    break
            
            # Adresse - chercher des patterns d'adresse
            # Chercher des codes postaux fran√ßais
            cp_match = re.search(r'\d{5}\s+[\w-]+', text)
            if cp_match:
                section['adresse'] = cp_match.group()
            
            if section['email'] or section['telephone']:
                sections.append(section)
    
    if sections:
        clubs = sections
        print(f"   ‚úÖ {len(clubs)} clubs extraits par m√©thode titres")
    else:
        print("   ‚ö†Ô∏è M√©thode titres n'a rien trouv√©, analyse manuelle du HTML n√©cessaire")
        print("   Consultez retrocalage_debug.html pour voir la structure")
    
    return clubs


def save_to_csv(clubs, filename):
    """
    Sauvegarde les clubs dans un fichier CSV.
    """
    if not clubs:
        print("‚ö†Ô∏è Aucun club √† sauvegarder")
        return
    
    print()
    print(f"üíæ Sauvegarde dans {filename}...")
    
    fieldnames = ['nom', 'adresse', 'representant', 'telephone', 'email', 'site']
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(clubs)
    
    print(f"‚úÖ {len(clubs)} clubs sauvegard√©s!")


# =============================================================================
# MAIN
# =============================================================================

def main():
    start_time = datetime.now()
    
    # √âtape 1: Charger toutes les donn√©es avec Selenium
    html = load_all_clubs()
    
    # √âtape 2: Extraire les donn√©es avec BeautifulSoup
    clubs = extract_clubs(html)
    
    # √âtape 3: Sauvegarder
    save_to_csv(clubs, OUTPUT_FILE)
    
    # R√©sum√©
    duration = datetime.now() - start_time
    print()
    print("=" * 60)
    print(f"üèÅ Termin√© en {duration.total_seconds():.1f} secondes")
    print(f"üìä {len(clubs)} clubs trouv√©s")
    print("=" * 60)


if __name__ == "__main__":
    main()

